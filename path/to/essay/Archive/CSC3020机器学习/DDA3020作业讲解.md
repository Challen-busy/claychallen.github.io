DDA3020作业

[TOC]



# Assignment 1

## ————————————————PROBLEM 1

------

![image-20240430233157167](https://raw.githubusercontent.com/Challen-busy/Typora_Picture/main/image-20240430233157167.png)

### 1. 正交矩阵orthogonal matrix证明

![image-20240430233438145](https://raw.githubusercontent.com/Challen-busy/Typora_Picture/main/image-20240430233438145.png)

#### 正交矩阵定义

正交矩阵：是指一个方阵  *Q*，满足 ==*Q^T^Q*=*QQ^T^*=*I* 的性质，其中 *Q^T^* 表示 *Q* 的转置，*I* 表示单位矩阵。正交矩阵的列向量组成了一个标准正交基。







### 2. 证明 正交矩阵orthognal matrix 的 特征值eigenvalue 必须是1.

![image-20240430233650249](https://raw.githubusercontent.com/Challen-busy/Typora_Picture/main/image-20240430233650249.png)

#### 特征值Eigenvalue定义

在线性代数中，给定一个方阵 Q，如果存在一个非零向量 *v* 和一个标量 *λ* 使得Qv=λv，那么我们称 *λ* 是矩阵 Q 的一个特征值，而 v称为对应于 λ的特征向量。

#### 范数Norm定义

向量范数和矩阵范数都是用来衡量向量和矩阵“大小”或“长度”的工具，但它们在定义和应用方面有一些差异。

![image-20240430234721134](https://raw.githubusercontent.com/Challen-busy/Typora_Picture/main/image-20240430234721134.png)

欧几里得矩阵范数：

矩阵的欧几里得范数，也被称为Frobenius范数，是一种用于衡量矩阵“大小”或“复杂度”的工具。具体来说，它测量的是矩阵中所有元素的平方和的平方根。公式表示为：

![image-20240430235108027](https://raw.githubusercontent.com/Challen-busy/Typora_Picture/main/image-20240430235108027.png)

通俗比喻：

可以将Frobenius范数比喻为评估一个学校的整体表现。想象一个学校里有多个班级（矩阵的行），每个班级有多名学生（矩阵的列），每名学生的成绩就是矩阵中的一个元素。Frobenius范数通过先平方每个学生的成绩，然后加总所有学生的成绩，最后取平方根，从而得到一个数值，这个数值反映了整个学校的总体学术水平。通过放大每个学生的成绩差异（平方），我们能更明显地看出学校在哪些方面做得好或不好。

![image-20240430234313438](https://raw.githubusercontent.com/Challen-busy/Typora_Picture/main/image-20240430234313438.png)

![image-20240501001129606](https://raw.githubusercontent.com/Challen-busy/Typora_Picture/main/image-20240501001129606.png)

------

## ————————————————PROBLEM 2

![image-20240430235351897](https://raw.githubusercontent.com/Challen-busy/Typora_Picture/main/image-20240430235351897.png)

## 3. Convex证明

![image-20240501000148435](https://raw.githubusercontent.com/Challen-busy/Typora_Picture/main/image-20240501000148435.png)

#### 凸性定义

![image-20240501001005990](https://raw.githubusercontent.com/Challen-busy/Typora_Picture/main/image-20240501001005990.png)



![image-20240501000857673](https://raw.githubusercontent.com/Challen-busy/Typora_Picture/main/image-20240501000857673.png)

#### 复合函数composite function的凸性证明

证明外函数是凸的（在定义域上），证明内函数是凸的。

## ————————————————PROBLEM 3

![image-20240501002134877](https://raw.githubusercontent.com/Challen-busy/Typora_Picture/main/image-20240501002134877.png)

## 4.熵、交叉熵、KL散度

![image-20240501003235237](https://raw.githubusercontent.com/Challen-busy/Typora_Picture/main/image-20240501003235237.png)

#### 熵与交叉熵

![image-20240501001554839](https://raw.githubusercontent.com/Challen-busy/Typora_Picture/main/image-20240501001554839.png)

#### KL散度

![image-20240501003015021](https://raw.githubusercontent.com/Challen-busy/Typora_Picture/main/image-20240501003015021.png)

## ————————————————PROBLEM 4

![image-20240510225233696](https://raw.githubusercontent.com/Challen-busy/Typora_Picture/main/image-20240510225233696.png)



## 5. 线性回归

![image-20240510225336485](https://raw.githubusercontent.com/Challen-busy/Typora_Picture/main/image-20240510225336485.png)

或者：

![image-20240510232958332](https://raw.githubusercontent.com/Challen-busy/Typora_Picture/main/image-20240510232958332.png)

![image-20240510233031155](https://raw.githubusercontent.com/Challen-busy/Typora_Picture/main/image-20240510233031155.png)











#### 闭式解

在数学中，**闭式解** (Closed-form Solution) 是指能够通过有限步的代数操作得到的解。即，不需要使用迭代算法，仅通过解析的代数方法就可以直接计算出结果。

![image-20240510232637371](https://raw.githubusercontent.com/Challen-busy/Typora_Picture/main/image-20240510232637371.png)

![image-20240510234046955](https://raw.githubusercontent.com/Challen-busy/Typora_Picture/main/image-20240510234046955.png)

#### 矩阵求导



#### 梯度下降

梯度下降法是一种迭代优化算法，无法直接得到解析解，必须通过迭代多次才能逐步逼近最优解。

![image-20240511000703553](https://raw.githubusercontent.com/Challen-busy/Typora_Picture/main/image-20240511000703553.png)

## 6. 最大似然估计MLE

## ————————————————PROBLEM 5

![image-20240511195449203](https://raw.githubusercontent.com/Challen-busy/Typora_Picture/main/image-20240511195449203.png)

![image-20240511195501904](https://raw.githubusercontent.com/Challen-busy/Typora_Picture/main/image-20240511195501904.png)

![image-20240511195534977](https://raw.githubusercontent.com/Challen-busy/Typora_Picture/main/image-20240511195534977.png)

## 7. SVM 平面

![image-20240511200054915](https://raw.githubusercontent.com/Challen-busy/Typora_Picture/main/image-20240511200054915.png)

![image-20240511210553053](https://raw.githubusercontent.com/Challen-busy/Typora_Picture/main/image-20240511210553053.png)









#### svm

![image-20240511204436806](https://raw.githubusercontent.com/Challen-busy/Typora_Picture/main/image-20240511204436806.png)

![image-20240515040119869](https://raw.githubusercontent.com/Challen-busy/Typora_Picture/main/image-20240515040119869.png)

![image-20240515040139054](https://raw.githubusercontent.com/Challen-busy/Typora_Picture/main/image-20240515040139054.png)

![image-20240515040010003](https://raw.githubusercontent.com/Challen-busy/Typora_Picture/main/image-20240515040010003.png)

![image-20240515040952688](https://raw.githubusercontent.com/Challen-busy/Typora_Picture/main/image-20240515040952688.png)





## ————————————————PROBLEM 6

## 8. SVR支持向量回归

![image-20240515041800975](https://raw.githubusercontent.com/Challen-busy/Typora_Picture/main/image-20240515041800975.png)

![image-20240515041812072](https://raw.githubusercontent.com/Challen-busy/Typora_Picture/main/image-20240515041812072.png)

![image-20240515041823396](https://raw.githubusercontent.com/Challen-busy/Typora_Picture/main/image-20240515041823396.png)





![image-20240515042208798](https://raw.githubusercontent.com/Challen-busy/Typora_Picture/main/image-20240515042208798.png)

**Large *C* - More complex model. Low Bias, High variance.**

**Small *C* - Less complex model. High Bias, Low Variance.**







#### KKT

KKT条件包括以下几部分：

1. **梯度条件（Stationarity conditions）**： 这些条件确保拉格朗日函数对每个变量的偏导数为零。
2. **可行性条件（Primal feasibility）**： 这些条件确保原始问题的约束条件被满足。
3. **对偶可行性条件（Dual feasibility）**： 这些条件确保拉格朗日乘数非负。
4. **互补松弛条件（Complementary slackness）**： 这些条件确保拉格朗日乘数与对应的约束条件之间的乘积为零。



![image-20240515042805135](https://raw.githubusercontent.com/Challen-busy/Typora_Picture/main/image-20240515042805135.png)



## ————————————————决策树预习

**决策树简介**

决策树是一种用于分类和回归的树状模型。在分类问题中，决策树通过一系列的决策节点将数据集划分成不同的类别。每个节点表示一个属性上的测试，每个分支代表这个属性的一个可能值，每个叶子节点代表一个类别。

**决策树的构建**

决策树的构建通常基于以下几种准则：

1. 信息增益（Information Gain）
2. 增益率（Gain Ratio）
3. 基尼指数（Gini Index）

![image-20240515043552184](https://raw.githubusercontent.com/Challen-busy/Typora_Picture/main/image-20240515043552184.png)

信息是有秩序的约束条件

g(D,A)信息增益Gain是某个信息约束对整体混乱程度的降低，也就是熵的降低。

![image-20240515220726320](https://raw.githubusercontent.com/Challen-busy/Typora_Picture/main/image-20240515220726320.png)



![image-20240515220802727](https://raw.githubusercontent.com/Challen-busy/Typora_Picture/main/image-20240515220802727.png)

![image-20240515220924857](https://raw.githubusercontent.com/Challen-busy/Typora_Picture/main/image-20240515220924857.png)



最后按照信息增益率来排序构建决策树。增益率越大，越优先。





## ————————————————CNN预习

#### 什么是卷积神经网络（CNN）？

卷积神经网络（CNN）是一种专门用于处理具有网格拓扑（如图像）的数据的深度学习模型。CNN在图像分类、目标检测和图像生成等任务中表现出色。CNN的主要组成部分包括卷积层、池化层（或子采样层）、全连接层和激活函数。

#### CNN的基本结构

1. **输入层（Input Layer）**：接收原始输入数据，例如一张32×32的图像。
2. **卷积层（Convolutional Layer）**：通过卷积操作提取输入数据的局部特征。卷积层的主要参数包括滤波器大小、步幅（stride）和填充（padding）。
3. **激活函数层（Activation Function Layer）**：通常使用ReLU（Rectified Linear Unit）激活函数，将非线性引入模型。
4. **池化层（Pooling Layer）**：通过降采样操作减少数据的尺寸，从而降低计算复杂度和过拟合风险。常用的池化方法是最大池化（Max Pooling）。
5. **全连接层（Fully Connected Layer）**：将前面提取的特征进行组合，用于最终的分类或回归任务。
6. **输出层（Output Layer）**：给出最终的预测结果，例如图像分类的类别概率。



#### 卷积层

![image-20240515232423610](https://raw.githubusercontent.com/Challen-busy/Typora_Picture/main/image-20240515232423610.png)
$$

$$

```
输入图像:
1 1 1 0 0
0 1 1 1 0
0 0 1 0 0
0 0 0 0 0
0 0 0 0 0

滤波器:
1 0 1
0 1 0
1 0 1

卷积结果:
3 2 3
2 4 2
1 2 1
```



#### 激活层

![image-20240515232724312](https://raw.githubusercontent.com/Challen-busy/Typora_Picture/main/image-20240515232724312.png)

#### 池化层

![image-20240515232738570](https://raw.githubusercontent.com/Challen-busy/Typora_Picture/main/image-20240515232738570.png)

#### 全连接层

![image-20240515232752484](https://raw.githubusercontent.com/Challen-busy/Typora_Picture/main/image-20240515232752484.png)





## KKT

![image-20240516053029387](https://raw.githubusercontent.com/Challen-busy/Typora_Picture/main/undefinedimage-20240516053029387.png)
